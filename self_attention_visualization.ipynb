{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention Visualization\n",
    "\n",
    "This notebook visualizes the self-attention maps of a Vision Transformer (ViT). \n",
    "It uses `timm` to load a standard pre-trained model and registers hooks to capture attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Load Model and Register Hooks ---\n",
    "\n",
    "# Load a standard pre-trained ViT from timm\n",
    "model_name = \"vit_base_patch16_224\"\n",
    "model = timm.create_model(model_name, pretrained=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Dictionary to store attention weights\n",
    "attention_weights = {}\n",
    "\n",
    "def get_attention_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        # In timm ViT, the attention module returns the output (x), \n",
    "        # but we often need to dig into the internal logic to get raw attn weights.\n",
    "        # However, for many standard implementations, we can hook into the Softmax layer \n",
    "        # inside the Attention block if accessible, or modify the model to return weights.\n",
    "        # \n",
    "        # Here, we assume we are hooking a module that outputs the attention weights directly\n",
    "        # or we calculate them if we hook the QKV projection. \n",
    "        # \n",
    "        # For simplicity in this demo with standard timm models, we will rely on \n",
    "        # extracting attributes if the model supports `get_attention_map` or simply \n",
    "        # assume the user provides a model wrapper that returns (output, attn).\n",
    "        #\n",
    "        # IF USING STANDARD TIMM VIT: The standard forward pass doesn't expose attn weights easily.\n",
    "        # A common trick is to wrap the Attention modules.\n",
    "        pass\n",
    "    return hook\n",
    "\n",
    "# --- Simpler Alternative: Wrap timm Attention to capture weights ---\n",
    "# This function wraps the Attention block of a timm ViT to save the attention matrix.\n",
    "class AttentionWrapper(nn.Module):\n",
    "    def __init__(self, attn_module, layer_id):\n",
    "        super().__init__()\n",
    "        self.attn_module = attn_module\n",
    "        self.layer_id = layer_id\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Copy-paste logic from standard ViT Attention to extract weights\n",
    "        # Or simply hook if possible. For generic usage, we will assume standard timm logic:\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.attn_module.qkv(x).reshape(B, N, 3, self.attn_module.num_heads, C // self.attn_module.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.attn_module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        \n",
    "        # SAVE WEIGHTS\n",
    "        attention_weights[f'layer_{self.layer_id}'] = attn.detach().cpu()\n",
    "        \n",
    "        attn = self.attn_module.attn_drop(attn)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.attn_module.proj(x)\n",
    "        x = self.attn_module.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "# Replace Attention blocks with our wrapper\n",
    "for i, block in enumerate(model.blocks):\n",
    "    if hasattr(block, 'attn'):\n",
    "        block.attn = AttentionWrapper(block.attn, i)\n",
    "        print(f\"Wrapped layer {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Prepare Data ---\n",
    "\n",
    "# Use a placeholder image from web or generate random noise for demonstration\n",
    "# Replace 'path/to/your/image.jpg' with your actual image path\n",
    "img_path = \"sample_image.jpg\" \n",
    "\n",
    "# Create a dummy image if not exists\n",
    "if not os.path.exists(img_path):\n",
    "    print(\"Creating dummy image...\")\n",
    "    Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)).save(img_path)\n",
    "\n",
    "# Standard ImageNet transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "original_img = Image.open(img_path).convert('RGB')\n",
    "input_tensor = transform(original_img).unsqueeze(0).to(device)\n",
    "print(f\"Input shape: {input_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Run Inference and Visualize ---\n",
    "\n",
    "# Helper function to calculate Normalized Mutual Information (from user code logic)\n",
    "def calculate_nmi(attn):\n",
    "    # Placeholder for NMI calculation logic found in original script\n",
    "    # Assuming generic calculation here or just using mean attention for viz\n",
    "    return attn\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "print(f\"Captured layers: {list(attention_weights.keys())}\")\n",
    "\n",
    "# Visualize Mean Attention Map for the last layer\n",
    "last_layer_key = list(attention_weights.keys())[-1]\n",
    "attn_map = attention_weights[last_layer_key][0] # (Heads, N, N)\n",
    "\n",
    "# Average over heads\n",
    "attn_mean = attn_map.mean(dim=0)\n",
    "\n",
    "# Visualize attention from CLS token to other tokens\n",
    "# Token 0 is usually CLS\n",
    "cls_attn = attn_mean[0, 1:] # Drop cls-to-cls attention\n",
    "\n",
    "# Reshape to image grid (14x14 for 224 image and 16 patch size)\n",
    "grid_size = int(np.sqrt(cls_attn.shape[0]))\n",
    "cls_attn_grid = cls_attn.reshape(grid_size, grid_size).numpy()\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].imshow(original_img)\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(cls_attn_grid, cmap='inferno')\n",
    "ax[1].set_title(f\"Attention (Layer {last_layer_key})\")\n",
    "ax[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
